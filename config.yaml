name: "en_to_kin_transformer"

data:
    train: "data/train"
    dev:   "data/dev"
    test:  "data/test"
    level: "bpe"
    sample_train_subset: -1             # Extend to longer sentences.
    sample_dev_subset: -1
    dataset_type: "plain"
    src:
        lang: "en"
        max_length: 30
        min_length: 1
        lowercase: False
        normalize: False
        level: "bpe"
        voc_limit: 400
        voc_min_freq: 1
        # voc_file: "data/vocab.en"
        tokenizer_type: "subword-nmt"
        tokenizer_cfg:
            num_merges: 200
            codes: "bpe.4000.codes"
            dropout: 0.0
            pretokenizer: "none"
    trg:
        lang: "kin"
        max_length: 30
        min_length: 1
        lowercase: False
        normalize: False
        level: "bpe"
        voc_limit: 400
        voc_min_freq: 1
        # voc_file: "data/vocab.kin"
        tokenizer_type: "subword-nmt"
        tokenizer_cfg:
            num_merges: 200
            codes: "bpe.4000.codes"
            dropout: 0.0
            pretokenizer: "none"

testing:
    n_best: 1
    beam_size: 5
    beam_alpha: 1.0
    batch_size: 10
    batch_type: "sentence"
    eval_metrics: ["bleu"]
    max_output_length: 31
    min_output_length: 1
    return_prob: "none"
    return_attention: False
    generate_unk: False
    no_repeat_ngram_size: -1
    repetition_penalty: -1
    sacrebleu_cfg:                      # sacrebleu options
        whitespace: False     # `remove_whitespace` option in sacrebleu.corpus_chrf() function (defalut: True)
        tokenize: "13a"            # `tokenize` option in sacrebleu.corpus_bleu() function (options include: "none" (use for already tokenized test data), "13a" (default minimal tokenizer), "intl" which mostly does punctuation and unicode, etc) 

training:
    reset_best_ckpt: False
    reset_scheduler: False
    reset_optimizer: False
    reset_iter_state: False
    random_seed: 42
    optimizer: "adam"
    adam_betas: [0.9, 0.999]
    learning_rate: 0.005
    learning_rate_min: 0.0001
    clip_grad_val: 1.0
    weight_decay: 0.
    loss: "crossentropy"
    label_smoothing: 0.0
    batch_size: 10
    batch_type: "sentence"
    batch_multiplier: 1
    normalization: "batch"
    scheduling: "plateau"
    patience: 5
    decrease_factor: 0.5
    epochs: 1
    updates: 100
    validation_freq: 10
    logging_freq: 10
    early_stopping_metric: "loss"
    model_dir: "models/en_to_kin_transformer"
    overwrite: False
    shuffle: True
    use_cuda: False
    fp16: True
    print_valid_sents: [0, 1, 2]
    keep_best_ckpts: 3

model:
    initializer: "xavier_uniform"
    init_gain: 1.0
    bias_initializer: "zeros"
    embed_initializer: "xavier_uniform"
    embed_init_gain: 1.0
    tied_embeddings: False       # Requires joint vocabulary.
    tied_softmax: True
    encoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4             # Increase to 8 for larger data.
        embeddings:
            embedding_dim: 256   # Increase to 512 for larger data.
            scale: True
            freeze: False
            dropout: 0.2
        # typically ff_size = 4 x hidden_size
        hidden_size: 256         # Increase to 512 for larger data.
        ff_size: 1024            # Increase to 2048 for larger data.
        dropout: 0.3
        freeze: False
        layer_norm: "pre"
    decoder:
        type: "transformer"
        num_layers: 6
        num_heads: 4              # Increase to 8 for larger data.
        embeddings:
            embedding_dim: 256    # Increase to 512 for larger data.
            scale: True
            freeze: False
            dropout: 0.2
        # typically ff_size = 4 x hidden_size
        hidden_size: 256         # TODO: Increase to 512 for larger data.
        ff_size: 1024            # TODO: Increase to 2048 for larger data.
        dropout: 0.3
        freeze: False
        layer_norm: "pre"